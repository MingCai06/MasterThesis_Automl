{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows the result of a parallel Bayesian Optimization over big data(>150M). The Parallel Bayesian Optimization strategy is called Multi-Points Expected Improvement using constant lier batch design. Method introduction see below.\n",
    "\n",
    "**Information of Dataset**<br>\n",
    "```\n",
    "+-------------------------------------------------------------------------------+\n",
    "| Name | Source | Size | TrainSet | TestSet | MissingValue | Balanced? |  Task  |\n",
    "|------+--------+------+----------+---------+--------------+-----------+--------+\n",
    "|albert| Openml |162.9M|  284910  | 140330  |     yes      |   yes     | binary |\n",
    "+-------------------------------------------------------------------------------+\n",
    "\n",
    "```\n",
    "**Environment**<br>\n",
    "To conduct this experiment, I run the code in Google Colab with 2 CPU.\n",
    "\n",
    "**Toal Runtime:** 12h<br>\n",
    "\n",
    "**Remark:**\n",
    "To speed up the process I **only use LightGBM** with 7 hyperparameters.  <br>\n",
    "\n",
    "**Experiment introduction** <br>\n",
    "All strategies have run 20 times\n",
    "- `njob_npoints10`: Using Parallel Optimization, batch size is 10\n",
    "- `njon_npoints1`: Using Parallel Optimization, batch size is 1\n",
    "- `with model_parallel`: only run two Surrogate Model(GP & RF) in parallel(CPU =2)\n",
    "- `without_parallel`: No Parallel, but LightGBM algorithms run parallel (Remark: not the evaluation of LightGBM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "pRt82cNZTF4G",
    "outputId": "898985c3-6d93-4537-f3b6-fdc08903bc85",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>total_time(s)</th>\n",
       "      <th>best_score</th>\n",
       "      <th>GP_time</th>\n",
       "      <th>GP_score</th>\n",
       "      <th>RF_time</th>\n",
       "      <th>RF_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>njob_npoints10</td>\n",
       "      <td>8809</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>5196</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>3690</td>\n",
       "      <td>0.7560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>njob_npoints1</td>\n",
       "      <td>8887</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>5311</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>3575</td>\n",
       "      <td>0.7560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model_parallel</td>\n",
       "      <td>12256</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>11383</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>12256</td>\n",
       "      <td>0.7596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>without_parallel</td>\n",
       "      <td>12752</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>6042</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>6710</td>\n",
       "      <td>0.7596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           strategy  total_time(s)  best_score  GP_time  GP_score  RF_time  \\\n",
       "2    njob_npoints10           8809      0.7592     5196    0.7592     3690   \n",
       "1     njob_npoints1           8887      0.7592     5311    0.7592     3575   \n",
       "3    model_parallel          12256      0.7596    11383    0.7586    12256   \n",
       "0  without_parallel          12752      0.7596     6042    0.7586     6710   \n",
       "\n",
       "   RF_score  \n",
       "2    0.7560  \n",
       "1    0.7560  \n",
       "3    0.7596  \n",
       "0    0.7596  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the result\n",
    "import pandas as pd\n",
    "'''\n",
    "The table should be generated automatically. However, there is a max limit time (90min) of using Colab, \n",
    "so in this experiment I just set it by hand. Good news is, I just wirte a dump function. so that laterly\n",
    "all the experiment results can be dumped locally. \n",
    "Due to the looooooong runtime, I don't dump the results this time. :)\n",
    "'''\n",
    "\n",
    "compare_result_ = pd.DataFrame(columns=['strategy','total_time(s)','best_score' ,'GP_time', 'GP_score','RF_time','RF_score'],\n",
    "                              data =[['without_parallel',12752,0.7596,6042,0.7586,6710,0.7596],\n",
    "                                     ['njob_npoints1',8887,0.7592,5311,0.7592,3575,0.756],\n",
    "                                     ['njob_npoints10',8809,0.7592,5196,0.7592,3690,0.756],\n",
    "                                     ['model_parallel',12256,0.7596,11383,0.7586,12256,0.7596]])\n",
    "compare_result_.sort_values(by=['total_time(s)'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48dkbPDfKwRv"
   },
   "source": [
    "<font color=DarkSlateBlue size=5>Result analysis</font> <br>\n",
    "\n",
    "From the result, we can see that the strategy: njob_npoints10  and strategy: njob_npoints1 outperform the other two strategies, which means Parallel Optimization by using Constant Lier has contributed to accelerating BO. <br>\n",
    "\n",
    "In this case, the number of CPU is 2. If we just run the `Parallel Optimization Strategy`, it is ok. But if we want to parallel running `2 Surrogate model` and `Parallel Optimization Strategy`, 2 CPU are far from enough to use, that is,there should be at least 4 CPU available. We can image if there are enough CPU cores, the performance could be better.\n",
    "\n",
    "More details please see Section 3-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=DarkSlateBlue size=5> Overall Introduction of Parallel Optimization by using Constant Lier</font>\n",
    "\n",
    "- A Optimizer (BO) is created, which is called Optimizer A.\n",
    "- Some initial samples(here 10) will be sampled randomly, and approximate the objective function f with a Surrogate model.\n",
    "- A copy of optimizer A (called Optimizer B) is created, which is then asked for a point, and the point is told to the  Optimizer B with some fake objective (lie),which is used with lie objective value being minimum of observed objective values. The next point is asked from copy, it is also told to the copy with fake objective and so on, until [$\\mathbf{X}_{i}$,$\\mathbf{ylie}_{i}$] for $i = 1,2,...q$ are found.<br>\n",
    "- Discard the copy optimizer B and evaluate all the q points by using objective function f, get the true [$\\mathbf{X}_{i}$,$\\mathbf{y}_{i}$] for $i = 1,2,...q$\n",
    "- Find the next sampling point $\\mathbf{X}_{j}$ by optimizing the acquisition function over the GP with all obeservated points (initial points + q points)\n",
    "- Repeat ...\n",
    "<br>\n",
    "\n",
    "**Parallel Optimization**\n",
    "```\n",
    "Input: Function f, Current dataset (X,y)\n",
    "Output: Global minimum x_min = argmin f\n",
    "1  for Surrogate model in [GP, RF]\n",
    "2  |  while (pending iterations):\n",
    "3  |  |  Create a new copy of optimizer 'opt'\n",
    "4  |  |  ......\n",
    "5  |  |  Through Constant liar Batch design strategies, \n",
    "6  |  |  accumulate p points X_new=(x_1..x_p).\n",
    "7  |  |  ......\n",
    "8  |  |  Discard 'opt'\n",
    "9  |  |  Evaluate f(X_new) in parallel get y_new\n",
    "10 |  |  Append (X_new,y_new) to (X,y)\n",
    "11 |  |  Fit updated model, discard opt\n",
    "12 |  end while (**having k batches/k*p points evaluated now**)\n",
    "13   end for\n",
    "14 return global minimum x_min = argmin y\n",
    "```\n",
    "**Constant liar Batch Design**\n",
    "```\n",
    "Input: Copied optimizer 'opt'\n",
    "Output: A batch of p points X_new=(x_1..x_p)\n",
    "1  while (pending points to sample):\n",
    "2  |  Choose next point x_i=argmin EI(X)\n",
    "3  |  Find current best y_opt\n",
    "4  |  Lie with constant y_i= min(y_opt)\n",
    "5  |  Append (x_i,y_i) to (X,y) within 'opt'\n",
    "6  |  Fit the copied model with (X,y) in 'opt'\n",
    "7  end while\n",
    "```\n",
    "\n",
    "**More information see the paper:**\n",
    "> https://hal.archives-ouvertes.fr/hal-00732512/document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Setting for Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct this experiment, I run the code in Google Colab with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZDUmKmFsKboc",
    "outputId": "818516b6-efc0-423a-8533-e8fd8f20c93e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Package 'python-software-properties' has no installation candidate\n",
      "Selecting previously unselected package google-drive-ocamlfuse.\n",
      "(Reading database ... 130942 files and directories currently installed.)\n",
      "Preparing to unpack .../google-drive-ocamlfuse_0.7.4-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking google-drive-ocamlfuse (0.7.4-0ubuntu1~ubuntu18.04.1) ...\n",
      "Setting up google-drive-ocamlfuse (0.7.4-0ubuntu1~ubuntu18.04.1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "··········\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "Please enter the verification code: Access token retrieved correctly.\n",
      "automl\t\tDatasetDescription.xlsx  pip_v1.0.ipynb\n",
      "catboost_info\tdrive\t\t\t predictions\n",
      "checkpoint.pkl\tingestion_program\t save\n",
      "data\t\tpipeline.py\t\t SKOPT_Controll_Cost.ipynb\n",
      "data_all\tpip_gpu_v2.0.ipynb\t Test_automl_Framework.ipynb\n",
      "Collecting scikit-optimize\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.16.4)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.13.2)\n",
      "Installing collected packages: scikit-optimize\n",
      "Successfully installed scikit-optimize-0.5.2\n",
      "Collecting scikit-learn==0.20.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/82/c0de5839d613b82bddd088599ac0bbfbbbcbd8ca470680658352d2c435bd/scikit_learn-0.20.3-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K     |████████████████████████████████| 5.4MB 9.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.20.3) (1.16.4)\n",
      "Installing collected packages: scikit-learn\n",
      "  Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\n",
      "Successfully installed scikit-learn-0.20.3\n"
     ]
    }
   ],
   "source": [
    "#default setting\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse -o nonempty drive\n",
    "\n",
    "import os\n",
    "path = \"drive/Masterarbeit/Automl_Framework/\"\n",
    "os.chdir(path)\n",
    "#os.listdir(path)\n",
    "!ls\n",
    "\n",
    "!pip install scikit-optimize\n",
    "!pip install scikit-learn==0.20.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "EyKlXR3xNxpb",
    "outputId": "f67eaebb-68ad-4271-d5fb-efc666c461e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '/content/drive/Masterarbeit/Automl_Framework/data', 'output': '/content/drive/Masterarbeit/Automl_Framework/save', 'program': '/content/drive/Masterarbeit/Automl_Framework/ingestion_program', 'submission': '/content/drive/Masterarbeit/Automl_Framework/automl'}\n",
      "INFO  [06-21 14:04:06] Read data: albert.csv\n",
      "\n",
      "--------Start [read_split]:\n",
      "------------Start [pre_clean]:\n",
      "\n",
      "reading csv : albert.csv ...\n",
      "cleaning data ...\n",
      "------------End   [pre_clean]. Time elapsed: 72.53 sec.\n",
      "\n",
      "> Number of common features : 78\n",
      "\n",
      "gathering and crunching for train and test datasets ...\n",
      "reindexing for train and test datasets ...\n",
      "dropping training duplicates ...\n",
      "dropping constant variables on training set ...\n",
      "\n",
      "> % missing values on train set:\n",
      "V67    0.8\n",
      "V64    0.8\n",
      "V12    0.8\n",
      "V53    0.8\n",
      "V69    0.8\n",
      "dtype: float64\n",
      "\n",
      "dropping columns with high missing rate >0.8...\n",
      "> No need to dropping!\n",
      "\n",
      "> Number of categorical features: 0\n",
      "> Number of numerical features: 78\n",
      "> Number of training samples : 284910\n",
      "> Number of test samples : 140330\n",
      "\n",
      "> Task : classification\n",
      "Train Traget\n",
      "0.0    142473\n",
      "1.0    142437\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Test Traget\n",
      "1.0    70183\n",
      "0.0    70147\n",
      "Name: class, dtype: int64\n",
      "\n",
      "encoding target ...\n",
      "training set encoding finished\n",
      "test set encoding finished\n",
      "\n",
      "Impute the Missing Values...\n",
      "\n",
      "--------End   [read_split]. Time elapsed: 77.91 sec.\n",
      "dict_keys(['train', 'test', 'target', 'y_test'])\n"
     ]
    }
   ],
   "source": [
    "# set path\n",
    "import numpy as np\n",
    "import base64\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "\n",
    "def mprint(msg):\n",
    "    from datetime import datetime\n",
    "    \"\"\"info\"\"\"\n",
    "    cur_time = datetime.now().strftime('%m-%d %H:%M:%S')\n",
    "    print(f\"INFO  [{cur_time}] {msg}\")\n",
    "\n",
    "def Config_DIRS():\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        # default local\n",
    "        ROOT_DIR = os.getcwd()\n",
    "        DIRS = {\n",
    "            'input': join(ROOT_DIR, 'data'),\n",
    "            'output': join(ROOT_DIR, 'save'),\n",
    "            'program': join(ROOT_DIR, 'ingestion_program'),\n",
    "            'submission': join(ROOT_DIR, 'automl')\n",
    "        }\n",
    "    elif len(sys.argv) == 3:\n",
    "        # default local\n",
    "        ROOT_DIR = os.getcwd()\n",
    "        DIRS = {\n",
    "            'input': join(ROOT_DIR, 'data'),\n",
    "            'output': join(ROOT_DIR, 'save'),\n",
    "            'program': join(ROOT_DIR, 'ingestion_program'),\n",
    "            'submission': join(ROOT_DIR, 'automl')\n",
    "        }\n",
    "\n",
    "    elif len(sys.argv) == 5:\n",
    "        # run in codalab\n",
    "        DIRS = {\n",
    "            'input': sys.argv[1],\n",
    "            'output': sys.argv[2],\n",
    "            'program': sys.argv[3],\n",
    "            'submission': sys.argv[4]\n",
    "        }\n",
    "    elif len(sys.argv) == 6 and sys.argv[1] == 'local':\n",
    "        # full call in local\n",
    "        DIRS = {\n",
    "            'input': sys.argv[2],\n",
    "            'output': sys.argv[3],\n",
    "            'program': sys.argv[4],\n",
    "            'submission': sys.argv[5]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Wrong number of arguments\")\n",
    "    sys.path.append(DIRS['submission'])\n",
    "    print(DIRS)\n",
    "    return(DIRS)\n",
    "\n",
    "DIRS = Config_DIRS()\n",
    "\n",
    "# read data\n",
    "import reader\n",
    "import imputation \n",
    "import encoder \n",
    "\n",
    "\n",
    "info = {\n",
    "\t\"table_sep\" : ',',\n",
    "\t\"target_name\" : 'class',\n",
    "\t\"miss_values\":'?'\n",
    "}\n",
    "\n",
    "\n",
    "datanames = [f for f in os.listdir(DIRS['input']) if not f.startswith('.')]\n",
    "reader = reader.Reader(sep = info['table_sep'],\n",
    "\t\t\t\t\t\tmiss_values=info['miss_values']\n",
    "\t\t\t\t\t\t)\n",
    "\n",
    "for dataname in datanames:\n",
    "        mprint(f'Read data: {dataname}')\n",
    "        datapath = join(DIRS['input'], dataname)\n",
    "\n",
    "data = reader.read_split([datapath],target_name=info['target_name'])\n",
    "print(data.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Default Function for Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "yKXy4z99iFed"
   },
   "outputs": [],
   "source": [
    "# import library and set the default function fot optimizer\n",
    "import lightgbm as lgb\n",
    "# Importing core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import pprint\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppressing warnings because of skopt verbosity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Classifiers\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Skopt functions\n",
    "from skopt import BayesSearchCV\n",
    "from skopt import gp_minimize # Bayesian optimization using Gaussian Processes\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\n",
    "from skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\n",
    "from skopt.callbacks import VerboseCallback # Callback to control the verbosity\n",
    "from skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta\n",
    "\n",
    "random_state = 42\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "roc_auc = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import encoder\n",
    "from util import timeit\n",
    "\n",
    "enc =encoder.Categorical_encoder()\n",
    "y=data['target']\n",
    "X = enc.fit_transform(data['train'],y)\n",
    "\n",
    "\n",
    "### default Function\n",
    "  \n",
    "def dump_result(data,inputname):\n",
    "    import time\n",
    "    from sklearn.externals.joblib import dump, load\n",
    "    path = DIRS['output']\n",
    "    datanames = sorted(os.listdir(DIRS['input']))[0].split('.')[0]\n",
    "  #  timestr = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "  #  filename = datanames + timestr + '.json'\n",
    "    filename = datanames +'-'+inputname + '.json'\n",
    "    dump(data, (path + '/' + filename))\n",
    "\n",
    "\n",
    "def load_result(filename):\n",
    "    dirs = init_dirs()\n",
    "    path = join(dirs['output'])\n",
    "    return load(path + '/' + filename)\n",
    "  \n",
    "# Callback function\n",
    "def on_step(optim_result):\n",
    "    score = opt.best_score_\n",
    "    score_std = opt.cv_results_['std_test_score'][opt.best_index_]\n",
    "    if score >= 0.99:\n",
    "        print('Best Score >0.99,Interrupting!')\n",
    "        return True\n",
    "\n",
    "# Reporting util for different optimizers\n",
    "def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "    \"\"\"\n",
    "    A wrapper for measuring time and performances of different optmizers\n",
    "    \n",
    "    optimizer = a sklearn or a skopt optimizer\n",
    "    X = the training set \n",
    "    y = our target\n",
    "    title = a string label for the experiment\n",
    "    \"\"\"\n",
    "\n",
    "    start = time()\n",
    "    if callbacks:\n",
    "        mprint(f'start tuning {title}...')\n",
    "        optimizer.fit(X, y, callback=callbacks)\n",
    "    else:\n",
    "    \n",
    "        mprint(f'start tuning {title}...')\n",
    "        optimizer.fit(X, y)\n",
    "\n",
    "    time_cost = time() - start\n",
    "    result = {}\n",
    "    result['best_score'] = optimizer.best_score_\n",
    "    result['best_score_std'] = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n",
    "    result['best_params'] = optimizer.best_params_\n",
    "    result['params'] = optimizer.cv_results_['params']\n",
    "    result['time_cost(s)'] = round(time_cost, 1)\n",
    "    result['all_cv_results'] = optimizer.cv_results_['mean_test_score'][:]\n",
    "    result['mean_score_time'] = optimizer.cv_results_['mean_score_time'][:]\n",
    "    result['mean_score_time'] = optimizer.cv_results_['mean_score_time'][:]\n",
    "\n",
    "    print('>' + title + ':')\n",
    "    time_cost = round(result['time_cost(s)'], 1)\n",
    "    cand = len(result['all_cv_results'])\n",
    "    best_cv = round(result['best_score'], 4)\n",
    "    best_cv_sd = round(result['best_score_std'], 4)\n",
    "    print(f'took{time_cost}s, candidates checked:{cand},best CV score: {best_cv} \\u00B1 {best_cv_sd}')\n",
    "    print(\"\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO-ouMtE1CYq"
   },
   "source": [
    "## without parellel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "GxGe2_75qs43",
    "outputId": "0deb2ed4-5524-4f61-dc49-164814e0361c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start [test_without_parallel]:\n",
      "INFO  [06-20 19:04:01] start tuning BayesSearchCV_GP...\n",
      ">BayesSearchCV_GP:\n",
      "took6042.2s, candidates checked:20,best CV score: 0.7586 ± 0.0019\n",
      "\n",
      "INFO  [06-20 20:44:43] start tuning BayesSearchCV_RF...\n",
      ">BayesSearchCV_RF:\n",
      "took6710.7s, candidates checked:20,best CV score: 0.7596 ± 0.0016\n",
      "\n",
      "End   [test_without_parallel]. Time elapsed: 12752.83 sec.\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline\n",
    "pipe = Pipeline([('model', lgb.LGBMClassifier(objective='binary', random_state=0))])\n",
    "\n",
    "# Define search space for LGB:\n",
    "search_space_LGB  = {\"model\": Categorical([lgb.LGBMClassifier(objective='binary', random_state=0)]),\n",
    "                     \"model__class_weight\":Categorical(categories=['balanced', None]),\n",
    "                    \"model__learning_rate\": Real(0.01, 1.0),\n",
    "                    \"model__boosting_type\": Categorical(categories=['gbdt', 'dart']),\n",
    "                    \"model__n_estimators\": Integer(10, 500),\n",
    "                    \"model__min_samples_split\": Integer(2, 10),\n",
    "                    \"model__min_samples_leaf\": Integer(1, 10),\n",
    "                    \"model__min_child_weight\": Integer(0, 50)}\n",
    "\n",
    "@timeit\n",
    "def test_without_parallel():\n",
    "    \n",
    "\n",
    "    final_result = {}\n",
    "    for baseEstimator in ['GP','RF']:    \n",
    "        opt = BayesSearchCV(pipe,\n",
    "                          search_spaces=[(search_space_LGB, 20)],\n",
    "                          scoring=roc_auc,\n",
    "                          cv=skf,\n",
    "                          n_jobs=1,\n",
    "                          return_train_score=False,\n",
    "                          optimizer_kwargs={'base_estimator': baseEstimator,\n",
    "                                           \"acq_func\" : \"EI\"},\n",
    "                          random_state=4)\n",
    "\n",
    "        result = report_perf(opt, X, y,'BayesSearchCV_'+baseEstimator,\n",
    "                                                         callbacks=[DeltaXStopper(0.0001)]\n",
    "                                                        )\n",
    "        final_result[baseEstimator] = result\n",
    "    return final_result\n",
    "ttt1 = test_without_parallel()\n",
    "save_result(ttt1,\"test_without_parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8jWYAED1MXZ"
   },
   "source": [
    "## with parallel n_jobs = -1,n_points =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BgynpsxO1Gpg",
    "outputId": "7c95fdc0-1ff7-41c9-a4c2-81ea7d18bde5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start [test_withnjob]:\n",
      "INFO  [06-20 22:36:34] start tuning BayesSearchCV_GP...\n",
      ">BayesSearchCV_GP:\n",
      "took5311.3s, candidates checked:20,best CV score: 0.7592 ± 0.0022\n",
      "\n",
      "INFO  [06-21 00:05:05] start tuning BayesSearchCV_RF...\n",
      ">BayesSearchCV_RF:\n",
      "took3575.8s, candidates checked:20,best CV score: 0.756 ± 0.0015\n",
      "\n",
      "End   [test_withnjob]. Time elapsed: 8887.07 sec.\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline \n",
    "pipe = Pipeline([('model', lgb.LGBMClassifier(objective='binary', random_state=0))])\n",
    "\n",
    "# Define search space for LGB:\n",
    "search_space_LGB  = {\"model\": Categorical([lgb.LGBMClassifier(objective='binary', random_state=0)]),\n",
    "                     \"model__class_weight\":Categorical(categories=['balanced', None]),\n",
    "                    \"model__learning_rate\": Real(0.01, 1.0),\n",
    "                    \"model__boosting_type\": Categorical(categories=['gbdt', 'dart']),\n",
    "                    \"model__n_estimators\": Integer(10, 500),\n",
    "                    \"model__min_samples_split\": Integer(2, 10),\n",
    "                    \"model__min_samples_leaf\": Integer(1, 10),\n",
    "                    \"model__min_child_weight\": Integer(0, 50)}\n",
    "\n",
    "@timeit\n",
    "def test_withnjob():\n",
    "    final_result = {}\n",
    "    for baseEstimator in ['GP','RF']:    \n",
    "        opt = BayesSearchCV(pipe,\n",
    "                          search_spaces=[(search_space_LGB, 20)],\n",
    "                          scoring=roc_auc,\n",
    "                          cv=skf,\n",
    "                          n_jobs=-1,\n",
    "                          n_points = 10,\n",
    "                          return_train_score=False,\n",
    "                          optimizer_kwargs={'base_estimator': baseEstimator,\n",
    "                                           \"acq_func\" : \"EI\"},\n",
    "                          random_state=4)\n",
    "\n",
    "        result = report_perf(opt, X, y,'BayesSearchCV_'+baseEstimator,\n",
    "                                                         callbacks=[DeltaXStopper(0.0001)]\n",
    "                                                        )\n",
    "        final_result[baseEstimator] = result\n",
    "    return final_result\n",
    "test_withnjob = test_withnjob()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "auI_xc-W1Xtz"
   },
   "source": [
    "## with njobs=-1,n_points=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "code",
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "2WBv5OxCDJ12",
    "outputId": "23567b73-74ab-49cb-d91c-a2f6d549edf6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Start [test_njob_npoint]:\n",
      "INFO  [06-21 14:17:23] start tuning BayesSearchCV_GP...\n",
      ">BayesSearchCV_GP:\n",
      "took5196.0s, candidates checked:20,best CV score: 0.7592 ± 0.0022\n",
      "\n",
      "INFO  [06-21 15:43:59] start tuning BayesSearchCV_RF...\n",
      ">BayesSearchCV_RF:\n",
      "took3690.4s, candidates checked:20,best CV score: 0.756 ± 0.0015\n",
      "\n",
      "--------End   [test_njob_npoint]. Time elapsed: 8886.41 sec.\n"
     ]
    }
   ],
   "source": [
    "# Initialize pipeline \n",
    "pipe = Pipeline([('model', lgb.LGBMClassifier(objective='binary', random_state=0))])\n",
    "\n",
    "# Define search space for LGB:\n",
    "search_space_LGB  = {\"model\": Categorical([lgb.LGBMClassifier(objective='binary', random_state=0)]),\n",
    "                     \"model__class_weight\":Categorical(categories=['balanced', None]),\n",
    "                    \"model__learning_rate\": Real(0.01, 1.0),\n",
    "                    \"model__boosting_type\": Categorical(categories=['gbdt', 'dart']),\n",
    "                    \"model__n_estimators\": Integer(10, 500),\n",
    "                    \"model__min_samples_split\": Integer(2, 10),\n",
    "                    \"model__min_samples_leaf\": Integer(1, 10),\n",
    "                    \"model__min_child_weight\": Integer(0, 50)}\n",
    "\n",
    "\n",
    "@timeit\n",
    "def test_njob_npoint():\n",
    "    final_result = {}\n",
    "    for baseEstimator in ['GP','RF']:    \n",
    "        opt = BayesSearchCV(pipe,\n",
    "                          search_spaces=[(search_space_LGB, 20)],\n",
    "                          scoring=roc_auc,\n",
    "                          cv=skf,\n",
    "                          n_jobs=-1,\n",
    "                          n_points = 10,\n",
    "                          return_train_score=False,\n",
    "                          optimizer_kwargs={'base_estimator': baseEstimator,\n",
    "                                           \"acq_func\" : \"EI\"},\n",
    "                          random_state=4)\n",
    "\n",
    "        result = report_perf(opt, X, y,'BayesSearchCV_'+baseEstimator,\n",
    "                                                         callbacks=[DeltaXStopper(0.0001)]\n",
    "                                                        )\n",
    "        final_result[baseEstimator] = result\n",
    "    return final_result\n",
    "njob_npoint = test_njob_npoint()\n",
    "#save_result(njob_npoint,\"test_njob_npoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkKv7JfeYBuL"
   },
   "outputs": [],
   "source": [
    "#dump the optimized results to .json \n",
    "dump_result(njob_npoint,'all_njob_npoint')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0GQGJFr1o6W"
   },
   "source": [
    "## with model_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "9OMMY0ekJvFu",
    "outputId": "23db5ae0-9742-4541-fd3f-27c7684d77fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Start [test_model_parallel]:\n",
      "--------End   [test_model_parallel]. Time elapsed: 12257.63 sec.\n",
      "Total time: 12257.6\n",
      "GP: 0.7585502020558024\n",
      "RF: 0.7596223157822072\n"
     ]
    }
   ],
   "source": [
    "# model parallel\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "ee=['GP', 'RF']\n",
    "def optm(baseEstimator):\n",
    "    # Callback function\n",
    "    def on_step(optim_result):\n",
    "        score = opt.best_score_\n",
    "        score_std = opt.cv_results_['std_test_score'][opt.best_index_]\n",
    "        if score >= 0.99:\n",
    "            print('Best Score >0.99,Interrupting!')\n",
    "            return True\n",
    "\n",
    "\n",
    "    # Reporting util for different optimizers\n",
    "    def report_perf(optimizer, X, y, title, callbacks=None):\n",
    "        \"\"\"\n",
    "        A wrapper for measuring time and performances of different optmizers\n",
    "\n",
    "        optimizer = a sklearn or a skopt optimizer\n",
    "        X = the training set \n",
    "        y = our target\n",
    "        title = a string label for the experiment\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        start = time()\n",
    "        if callbacks:\n",
    "            print(\"\")\n",
    "            mprint(f'start tuning {title}...')\n",
    "            optimizer.fit(X, y, callback=callbacks)\n",
    "        else:\n",
    "            print(\"\")\n",
    "            mprint(f'start tuning {title}...')\n",
    "            optimizer.fit(X, y)\n",
    "\n",
    "        time_cost = time() - start\n",
    "        result = {}\n",
    "        result['best_score'] = optimizer.best_score_\n",
    "        result['best_score_std'] = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n",
    "        result['best_params'] = optimizer.best_params_\n",
    "        result['params'] = optimizer.cv_results_['params']\n",
    "        result['time_cost(s)'] = round(time_cost, 1)\n",
    "        result['all_cv_results'] = optimizer.cv_results_['mean_test_score'][:]\n",
    "        result['mean_score_time'] = optimizer.cv_results_['mean_score_time'][:]\n",
    "        result['mean_score_time'] = optimizer.cv_results_['mean_score_time'][:]\n",
    "\n",
    "        print('>' + title + ':')\n",
    "        time_cost = round(result['time_cost(s)'], 1)\n",
    "        cand = len(result['best_params'])\n",
    "        best_cv = round(result['best_score'], 4)\n",
    "        best_cv_sd = round(result['best_score_std'], 4)\n",
    "        print(f'took{time_cost}s, candidates checked:{cand},best CV score: {best_cv} \\u00B1 {best_cv_sd}')\n",
    "        print(\"\")\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize a pipeline with a model\n",
    "    pipe = Pipeline([('model', lgb.LGBMClassifier(objective='binary', random_state=0))])\n",
    "\n",
    "    # Define search space for LGB:\n",
    "    search_space_LGB  = {\"model\": Categorical([lgb.LGBMClassifier(objective='binary', random_state=0)]),\n",
    "                       \"model__class_weight\":Categorical(categories=['balanced', None]),\n",
    "                      \"model__learning_rate\": Real(0.01, 1.0),\n",
    "                      \"model__boosting_type\": Categorical(categories=['gbdt', 'dart']),\n",
    "                      \"model__n_estimators\": Integer(10, 500),\n",
    "                      \"model__min_samples_split\": Integer(2, 10),\n",
    "                      \"model__min_samples_leaf\": Integer(1, 10),\n",
    "                      \"model__min_child_weight\": Integer(0, 50)}\n",
    "\n",
    "\n",
    "    r = {}\n",
    "  #  for baseEstimator in ['GP']:\n",
    "    opt = BayesSearchCV(pipe,\n",
    "                      search_spaces=[(search_space_LGB, 20)],\n",
    "                      scoring=roc_auc,\n",
    "                      cv=skf,\n",
    "                      n_jobs=1,\n",
    "  #                    n_points = 10,\n",
    "                      return_train_score=False,\n",
    "                      optimizer_kwargs={'base_estimator': baseEstimator,\n",
    "                                         \"acq_func\" : \"EI\"},\n",
    "                      random_state=4)\n",
    "\n",
    "    result = report_perf(opt, X, y,'BayesSearchCV_'+baseEstimator,\n",
    "                                                     callbacks=[DeltaXStopper(0.0001)]\n",
    "                                             )\n",
    "    r[baseEstimator] = result\n",
    "    return r\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "@timeit\n",
    "def test_model_parallel ():\n",
    "    start = time()\n",
    "    results = Parallel(n_jobs=-1,verbose=0)(delayed(optm)(baseEstimator) for baseEstimator in ee)\n",
    "    time_cost = time() - start\n",
    "    return results,time_cost\n",
    "\n",
    "model_parallel,time_cost = test_model_parallel()\n",
    "print('Total time:',round(time_cost,1))\n",
    "print('GP:' ,model_parallel[0]['GP']['best_score'])\n",
    "print('RF:' ,model_parallel[1]['RF']['best_score'])\n",
    "\n",
    "model_parallel_results={}\n",
    "model_parallel_results['GP'] = model_parallel[0]['GP']\n",
    "model_parallel_results['RF'] = model_parallel[1]['RF']\n",
    "model_parallel_results['Total_time_cost'] = round(time_cost,1)\n",
    "#model_parallel_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxKSGyRxRUhp"
   },
   "outputs": [],
   "source": [
    "#dump the optimized results to .json \n",
    "dump_result(model_parallel_results,'all_model_parallel_results')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "test_with_bigData_140M.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
